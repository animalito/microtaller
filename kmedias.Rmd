---
title: "K-medias"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, error = F, warning = F)
library(stringdist)
library(dplyr)
library(ggplot2)
library(tidyr)
library(knitr)
```

## Ejemplo clásico

```{r}
ggplot(filter(iris, Species %in% c('setosa','versicolor')), 
       aes(x=Sepal.Length, y=Petal.Width)) + geom_point()
```

## Ejemplo clásico

```{r}
ggplot(filter(iris, Species %in% c('setosa','versicolor')), 
       aes(x=Sepal.Length, y=Petal.Width, colour=Species)) + geom_point()
```

## Ejemplo más real (1/2)

```{r}
ggplot(airquality, aes(x=Ozone, y=Wind)) + geom_point()
```

donde no hay clusters bien definidos

## Ejemplo más real (2/2)

O donde tienes dimensión más alta (100 variables, 10 casos) observamos cosas como la siguiente:

```{r}
mat.1 <- matrix(rnorm(10*100), ncol=100)
dist(mat.1, method = 'euclidean')
```

## Enfoques basados en modelos

- Introducir variables latentes que expliquen diferencias en las distribuciones
de las variables observadas.
- Hay metodos *combinatorios* que usan las variables originales de manera directa
para tratar de segmentar las observaciones en grupos a través de los cuáles se
minimiza alguna función objetivo (e.g. minimizar la dispersión dentro de los 
grupos generados o maximizar la distancia entre los centroides de los grupos)

## K-medias: descripción 

- Fijamos el número $K$ de grupos que buscamos
- Supongamos que $C_1\cup\cdots\cup C_K$ es una partición de los
datos
- Sea $W(C_k)$ nuestra medida de variación dentro de los clusters. 
- Se busca resolver:

$$min_{C_1,\ldots, C_K} \sum_{k=1}^K W(C_k)$$ 

- Es un problema que no se puede resolver por enumeración pues el espacio 
de particiones posibles es muy grande.
- Sin embargo, si se escoge bien $W$ es posible conseguir un desempeño razonable (aunque, obvio la convergencia no está asegurada)

## K-medias: W 

- La definimos la medida de variación dentro de los clusters como el promedio de distancias
euclideanas al cuadrado dentro del cluster

$$W(C_k) =\frac{1}{|C_k|}\sum_{i,j\in C_k} ||x_i-x_j||^2,$$

De notar una serie de particularidades en esta ecuación, se extrae el 
algoritmo.

## K-medias: Algoritmo 

En el paso $s=1,2,\ldots$:

1. (cálculo de centroides) Dada una asignación a clusters, encontramos nuevos centros promediando en cada cluster :
$$m_k = \frac{1}{|C_k|}\sum_{i\in C_k} x_i.$$
2. Dadas las medias $m_k$  (que pensamos fijas),
encontramos una nueva asignación $C_k$ a clusters que minimice
$$ 2\sum_{k=1}^K \sum_{i\in C_k} ||x_i - m_k||^2,$$
y esto se hace asignando cada observación al centroide $m_k$ que esté más cercano.

Nos detenemos cuando los centroides se quedan casi fijos de una iteración a la siguiente.

## K-medias: observaciones

 - El algoritmo se puede arrancar con centroides escogidos al azar (puntos de datos escogidos al azar, por ejemplo).
 - Este algoritmo converge, pero no tiene garantía de obtener un mínimo global. Conviene correr varias veces, para distintos arranques aleatorios, y escoger
 la solución con función objetivo más chica. Cuando no es posible correrlo múltiples veces, puede ser que la solución obtenida esté muy lejos de una óptima.
 
## K-means: ejemplo 

Describiremos iteraciones para $k=5$ para el conjunto de datos:

```{r, fig.height=3, fig.width=5}
quakes.1 <- quakes[, c('lat','long')]
quakes.1$id <- 1:nrow(quakes.1)
ggplot(quakes.1, aes(x=lat, y=long)) + geom_point()
```

## K-means: ejemplo 

Seleccionamos muestra de datos al azar (centroides)
```{r, fig.height=3, fig.width=5}
set.seed(251122)
K <- 5
centros.1 <- sample_n(quakes.1, K) %>% 
  mutate(k = 1:K)
centros.2 <- centros.1 %>% gather(var, value.c, lat:long) %>% select(-id)
quakes.2 <- quakes.1 %>% gather(var, value, lat:long)

ggplot(quakes.1, aes(x=lat, y=long)) + geom_point() +
  geom_point(data = centros.1, aes(x=lat, y=long), size=7, colour='red')
```

## K-means: ejemplo 

Agrupamos:
```{r, fig.height=3, fig.width=5}
agrup.1 <- left_join(quakes.2, centros.2)
agrup <- agrup.1 %>% group_by(id, k) %>%
  summarise(dist=sum((value-value.c)^2)) %>% 
  group_by(id) %>%
  mutate(min.dist = min(dist)) %>%
  filter(min.dist == dist) %>%
  left_join(quakes.1)
ggplot(agrup, aes(x=lat, y=long, colour=factor(k))) + geom_point()
```

## K-means: ejemplo 

Recalculamos centros:
```{r, fig.height=3, fig.width=5}
centros.1 <- agrup %>% group_by(k) %>%
  summarise(lat=mean(lat), long=mean(long))
centros.2 <- centros.1 %>% gather(var, value.c, lat:long) 
ggplot(quakes.1, aes(x=lat, y=long)) + geom_point() +
  geom_point(data = centros.1, aes(x=lat, y=long), size=7, colour='red')
```

## K-means: ejemplo 

Agrupamos:
```{r, fig.height=3, fig.width=5}
agrup.1 <- left_join(quakes.2, centros.2)
agrup <- agrup.1 %>% group_by(id, k) %>%
  summarise(dist=sum((value-value.c)^2)) %>% 
  group_by(id) %>%
  mutate(min.dist = min(dist)) %>%
  filter(min.dist == dist) %>%
  left_join(quakes.1)
ggplot(agrup, aes(x=lat, y=long, colour=factor(k))) + geom_point()
```

## K-means: ejemplo 

Recalculamos centros:
```{r, fig.height=3, fig.width=5}
centros.1 <- agrup %>% group_by(k) %>%
  summarise(lat=mean(lat), long=mean(long))
centros.2 <- centros.1 %>% gather(var, value.c, lat:long) 
ggplot(quakes.1, aes(x=lat, y=long)) + geom_point() +
  geom_point(data = centros.1, aes(x=lat, y=long), size=7, colour='red')
```

## K-means: ejemplo 

Agrupamos:
```{r, fig.height=3, fig.width=5}
agrup.1 <- left_join(quakes.2, centros.2)
agrup <- agrup.1 %>% group_by(id, k) %>%
  summarise(dist=sum((value-value.c)^2)) %>% 
  group_by(id) %>%
  mutate(min.dist = min(dist)) %>%
  filter(min.dist == dist) %>%
  left_join(quakes.1)
ggplot(agrup, aes(x=lat, y=long, colour=factor(k))) + geom_point()
```

## Usando la funcion k-means 

```{r}
set.seed(2800)
k_medias <- kmeans(quakes.1[, c('lat','long')], centers = 5, nstart=30) 
# escoger varios comienzos aleatorios=
str(k_medias)
```

## Usando la funcion k-means 

```{r, fig.height=3, fig.width=5}
grupo <- k_medias$cluster
quakes.1$grupo <- grupo
ggplot(quakes.1, aes(x=lat, y=long, colour=factor(grupo))) + geom_point()
```

## ¿Cuándo usar o no usar k-medias? Existencia o no de grupos "naturales" 

Con iris es sencillo ver grupos "naturales", en un ejemplo como el siguiente no 
tanto.

```{r, fig.height=3, fig.width=5}
set.seed(90902)
df <- data.frame(x = rnorm(500,0,1), y = rnorm(500,0,1))
df$grupo <- kmeans(df, centers=5, nstart=20)$cluster
ggplot(df, aes(x=x, y=y, colour=factor(grupo))) + geom_point()
```

Nótese que k-means logró encontrar una buena solución, y esta solución puede
ser muy útil para nuestros fines (agrupa puntos "similares"). Sin embargo, en esta situación debemos reconocer que los tamaños, las posiciones,
y el número de grupos es fundamentalmente arbitrario, y una "buena" solución depende de nuestros fines.

## ¿Cuándo usar o no usar k-medias? 

Si corremos otra vez el algoritmo, vemos que los grupos encontrados son similares:
```{r, fig.height=3, fig.width=5}
df$grupo <- kmeans(df, centers=5, nstart=20)$cluster
ggplot(df, aes(x=x, y=y, colour=factor(grupo))) + geom_point()
```

## ¿Cuándo usar o no usar k-medias? 

```{r, fig.height=3, fig.width=5}
set.seed(909021)
df <- data.frame(x = rnorm(500,0,1), y = rnorm(500,0,1))
df$grupo <- kmeans(df, centers=5, nstart=20)$cluster
ggplot(df, aes(x=x, y=y, colour=factor(grupo))) + geom_point()
```

La solución es bastante diferente. Esta diferencia no se debe al comienzo aleatorio
del algoritmo. Se debe más bien a que los grupos se están definiendo por variación muestral, y pequeñas diferencias en las muestras.

En esta situación, debemos aceptar que la **responsabilidad** de escoger la solución
final está en nuestras manos, y no del algoritmo, y entender que hay arbitrariedad considerable en los segmentos encontrado sus tamaños. Esto no le quita necesariamente utilidad a la segmentación resultante, pero hay que recordar que los grupos que encontramos son en ciertos aspectos arbitrarios.

## Selección de número de clusters 

Podemos medir la calidad de la segmentación según la suma de cuadrados dentro
de los clusters, que nos dice qué tan compactos son. Primero vemos
un ejemplo simple

```{r, fig.height=3, fig.width=5}
set.seed(2800)
df <- data.frame(x=c(rnorm(100,-50,10), rnorm(100,0,10), rnorm(70,30,2) ))
qplot(df$x)
```

## Selección de número de clusters 

Agregar un cluster adicional hace más complejo nuestro resumen, así
que incrementamos el número de clusters sólo cuando tenemos una mejora
considerable en la solución.

```{r, fig.height=3, fig.width=5}
ajustes.km <- lapply(1:20, function(k){
  kmedias <- kmeans(df, centers = k, nstart = 20)    
  kmedias
})
tot.within <- sapply(ajustes.km, function(aj){ aj$tot.withinss})
qplot(1:length(tot.within), tot.within, geom='line') + geom_point()
```

En este caso, parece que 3 es un buen número.

## Caracterización y descripción de grupos 

Lo primero que tenemos que hacer para entender una segmentación dada es 
caracterizar a los grupos en cuanto a las variables que usamos para segmentar.